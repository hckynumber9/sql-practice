{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woot\n"
     ]
    }
   ],
   "source": [
    "# Establish Spark session\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[2]\") \\\n",
    "            .appName(\"df lecture\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext \n",
    "print(\"woot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SQL Window Functions** ##\n",
    "* Start spark session\n",
    "* Execute SQL queries\n",
    "* Next: Add notes and continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV\n",
    "df_sales = spark.read.csv('data/sales.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "# Now create an SQL table and issue SQL queries against it without\n",
    "# using the sqlContext but through the SparkSession object.\n",
    "# Creates a temporary view of the DataFrame\n",
    "df_sales.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql('''\n",
    "    SELECT state, AVG(amount) as avg_amount\n",
    "    FROM sales\n",
    "    GROUP BY state\n",
    "    ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV in a Spark dataframe\n",
    "\n",
    "df_bike = spark.read.csv('data/2012Q1-capitalbikeshare-tripdata.csv', header=True, quote='\"', sep=',', inferSchema=False)\n",
    "df_bike.createOrReplaceTempView('bike_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+-----------+--------------------+--------------------+------------------+--------------------+-----------+-----------+\n",
      "|duration_seconds| start_time|   end_time|start_station_number|       start_station|end_station_number|         end_station|bike_number|member_type|\n",
      "+----------------+-----------+-----------+--------------------+--------------------+------------------+--------------------+-----------+-----------+\n",
      "|             475|1/1/12 0:04|1/1/12 0:11|               31245|7th & R St NW / S...|             31109|       7th & T St NW|     W01412|     Member|\n",
      "|            1162|1/1/12 0:10|1/1/12 0:29|               31400|Georgia & New Ham...|             31103|16th & Harvard St NW|     W00524|     Casual|\n",
      "|            1145|1/1/12 0:10|1/1/12 0:29|               31400|Georgia & New Ham...|             31103|16th & Harvard St NW|     W00235|     Member|\n",
      "|             485|1/1/12 0:15|1/1/12 0:23|               31101|      14th & V St NW|             31602|Park Rd & Holmead...|     W00864|     Member|\n",
      "|             471|1/1/12 0:15|1/1/12 0:23|               31102| 11th & Kenyon St NW|             31109|       7th & T St NW|     W00995|     Member|\n",
      "+----------------+-----------+-----------+--------------------+--------------------+------------------+--------------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check to make sure it worked\n",
    "\n",
    "result = spark.sql('''\n",
    "    SELECT * FROM bike_data LIMIT 5\n",
    "    ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+\n",
      "|duration_seconds|running_total|\n",
      "+----------------+-------------+\n",
      "|             475|        475.0|\n",
      "|            1162|       2782.0|\n",
      "|            1145|       2782.0|\n",
      "|             485|       3738.0|\n",
      "|             471|       3738.0|\n",
      "|             358|       4096.0|\n",
      "|            1754|       5850.0|\n",
      "|             259|       6109.0|\n",
      "|             516|       6625.0|\n",
      "|             913|       7538.0|\n",
      "|            1097|       8635.0|\n",
      "|             490|       9125.0|\n",
      "|            1045|      11205.0|\n",
      "|            1035|      11205.0|\n",
      "|            1060|      14063.0|\n",
      "|            1039|      14063.0|\n",
      "|             443|      14063.0|\n",
      "|             316|      14063.0|\n",
      "|             506|      14569.0|\n",
      "|             956|      15525.0|\n",
      "+----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate basic window function.  'OVER' designates the window function, ordered by start_time.\n",
    "\n",
    "result = spark.sql('''\n",
    "    SELECT duration_seconds,\n",
    "       SUM(duration_seconds) OVER (ORDER BY start_time) AS running_total\n",
    "    FROM bike_data\n",
    "    ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-------------+\n",
      "|start_station_number|duration_seconds|running_total|\n",
      "+--------------------+----------------+-------------+\n",
      "|               31217|             841|       1613.0|\n",
      "|               31217|             772|       1613.0|\n",
      "|               31217|            1623|       3236.0|\n",
      "|               31217|            1260|       5751.0|\n",
      "|               31217|            1255|       5751.0|\n",
      "|               31217|            5154|      12076.0|\n",
      "|               31217|            1171|      12076.0|\n",
      "|               31217|            4880|      16956.0|\n",
      "|               31217|             531|      17487.0|\n",
      "|               31217|            8831|      26318.0|\n",
      "|               31217|            8684|      35002.0|\n",
      "|               31217|            8681|      43683.0|\n",
      "|               31217|            8528|      52211.0|\n",
      "|               31217|             881|      53092.0|\n",
      "|               31217|             858|      53950.0|\n",
      "|               31217|            3029|      56979.0|\n",
      "|               31217|            2097|      61158.0|\n",
      "|               31217|            2082|      61158.0|\n",
      "|               31217|            1997|      65114.0|\n",
      "|               31217|            1959|      65114.0|\n",
      "+--------------------+----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PARTITION BY separates by starting station, then performs running_total ordered by start_time.  running_total starts over at each new station.\n",
    "\n",
    "result = spark.sql('''\n",
    "   SELECT start_station_number,\n",
    "       duration_seconds,\n",
    "       SUM(duration_seconds) OVER\n",
    "         (PARTITION BY start_station_number ORDER BY start_time)\n",
    "         AS running_total\n",
    "   FROM bike_data\n",
    "   WHERE start_time < '2012-01-08'\n",
    "   ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-------------+\n",
      "|start_station_number|duration_seconds|running_total|\n",
      "+--------------------+----------------+-------------+\n",
      "|               31217|             841|    2726242.0|\n",
      "|               31217|             772|    2726242.0|\n",
      "|               31217|            1623|    2726242.0|\n",
      "|               31217|            1260|    2726242.0|\n",
      "|               31217|            1255|    2726242.0|\n",
      "|               31217|            5154|    2726242.0|\n",
      "|               31217|            1171|    2726242.0|\n",
      "|               31217|            4880|    2726242.0|\n",
      "|               31217|             531|    2726242.0|\n",
      "|               31217|            8831|    2726242.0|\n",
      "|               31217|            8684|    2726242.0|\n",
      "|               31217|            8681|    2726242.0|\n",
      "|               31217|            8528|    2726242.0|\n",
      "|               31217|             881|    2726242.0|\n",
      "|               31217|             858|    2726242.0|\n",
      "|               31217|            3029|    2726242.0|\n",
      "|               31217|            2097|    2726242.0|\n",
      "|               31217|            2082|    2726242.0|\n",
      "|               31217|            1997|    2726242.0|\n",
      "|               31217|            1959|    2726242.0|\n",
      "+--------------------+----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Without ORDER BY, running_total is the total of all seconds (see below without ORDER BY).  ORDER BY moves row by row in order of the designated column.  \n",
    "# ORDER BY and PARTITION define the window.  Can't include window function in a GROUP BY clause.\n",
    "\n",
    "result = spark.sql('''\n",
    "    SELECT start_station_number,\n",
    "       duration_seconds,\n",
    "       SUM(duration_seconds) OVER\n",
    "         (PARTITION BY start_station_number)\n",
    "         AS running_total\n",
    "   FROM bike_data\n",
    "   WHERE start_time < '2012-01-08'\n",
    "    ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-------------+-------------+------------------+\n",
      "|start_station_number|duration_seconds|running_total|running_count|       running_avg|\n",
      "+--------------------+----------------+-------------+-------------+------------------+\n",
      "|               31217|             841|       1613.0|            2|             806.5|\n",
      "|               31217|             772|       1613.0|            2|             806.5|\n",
      "|               31217|            1623|       3236.0|            3|1078.6666666666667|\n",
      "|               31217|            1260|       5751.0|            5|            1150.2|\n",
      "|               31217|            1255|       5751.0|            5|            1150.2|\n",
      "|               31217|            5154|      12076.0|            7| 1725.142857142857|\n",
      "|               31217|            1171|      12076.0|            7| 1725.142857142857|\n",
      "|               31217|            4880|      16956.0|            8|            2119.5|\n",
      "|               31217|             531|      17487.0|            9|            1943.0|\n",
      "|               31217|            8831|      26318.0|           10|            2631.8|\n",
      "|               31217|            8684|      35002.0|           11|            3182.0|\n",
      "|               31217|            8681|      43683.0|           12|           3640.25|\n",
      "|               31217|            8528|      52211.0|           13| 4016.230769230769|\n",
      "|               31217|             881|      53092.0|           14| 3792.285714285714|\n",
      "|               31217|             858|      53950.0|           15|3596.6666666666665|\n",
      "|               31217|            3029|      56979.0|           16|         3561.1875|\n",
      "|               31217|            2097|      61158.0|           18|3397.6666666666665|\n",
      "|               31217|            2082|      61158.0|           18|3397.6666666666665|\n",
      "|               31217|            1997|      65114.0|           20|            3255.7|\n",
      "|               31217|            1959|      65114.0|           20|            3255.7|\n",
      "+--------------------+----------------+-------------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using aggregates with window functions.\n",
    "\n",
    "result = spark.sql('''\n",
    "    SELECT start_station_number,\n",
    "           duration_seconds,\n",
    "           SUM(duration_seconds) OVER\n",
    "               (PARTITION BY start_station_number ORDER BY start_time) AS running_total,\n",
    "           COUNT(duration_seconds) OVER\n",
    "               (PARTITION BY start_station_number ORDER BY start_time) AS running_count,\n",
    "           AVG(duration_seconds) OVER\n",
    "               (PARTITION BY start_station_number ORDER BY start_time) AS running_avg\n",
    "    FROM bike_data\n",
    "    WHERE start_time < '2012-01-08'\n",
    "     ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------------+----------+\n",
      "|start_station_number| start_time|duration_seconds|row_number|\n",
      "+--------------------+-----------+----------------+----------+\n",
      "|               31245|1/1/12 0:04|             475|         1|\n",
      "|               31400|1/1/12 0:10|            1162|         2|\n",
      "|               31400|1/1/12 0:10|            1145|         3|\n",
      "|               31101|1/1/12 0:15|             485|         4|\n",
      "|               31102|1/1/12 0:15|             471|         5|\n",
      "|               31017|1/1/12 0:17|             358|         6|\n",
      "|               31236|1/1/12 0:18|            1754|         7|\n",
      "|               31101|1/1/12 0:22|             259|         8|\n",
      "|               31014|1/1/12 0:24|             516|         9|\n",
      "|               31101|1/1/12 0:25|             913|        10|\n",
      "|               31303|1/1/12 0:29|            1097|        11|\n",
      "|               31222|1/1/12 0:30|             490|        12|\n",
      "|               31230|1/1/12 0:32|            1045|        13|\n",
      "|               31107|1/1/12 0:32|            1035|        14|\n",
      "|               31107|1/1/12 0:33|            1060|        15|\n",
      "|               31230|1/1/12 0:33|            1039|        16|\n",
      "|               31237|1/1/12 0:33|             443|        17|\n",
      "|               31109|1/1/12 0:33|             316|        18|\n",
      "|               31203|1/1/12 0:34|             506|        19|\n",
      "|               31214|1/1/12 0:36|             956|        20|\n",
      "+--------------------+-----------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ROW_NUMBER() displays row numbers according to ORDER BY.  PARTITION BY will cause row numbers to reset at the partition.\n",
    "\n",
    "result = spark.sql('''\n",
    "   SELECT start_station_number,\n",
    "          start_time,\n",
    "          duration_seconds,\n",
    "          ROW_NUMBER() OVER (ORDER BY start_time) AS row_number\n",
    "   FROM bike_data\n",
    "   WHERE start_time < '2012-01-08'\n",
    "   ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+----+\n",
      "|start_station_number|duration_seconds|rank|\n",
      "+--------------------+----------------+----+\n",
      "|               31217|             841|   1|\n",
      "|               31217|             772|   1|\n",
      "|               31217|            1623|   3|\n",
      "|               31217|            1260|   4|\n",
      "|               31217|            1255|   4|\n",
      "|               31217|            5154|   6|\n",
      "|               31217|            1171|   6|\n",
      "|               31217|            4880|   8|\n",
      "|               31217|             531|   9|\n",
      "|               31217|            8831|  10|\n",
      "|               31217|            8684|  11|\n",
      "|               31217|            8681|  12|\n",
      "|               31217|            8528|  13|\n",
      "|               31217|             881|  14|\n",
      "|               31217|             858|  15|\n",
      "|               31217|            3029|  16|\n",
      "|               31217|            2097|  17|\n",
      "|               31217|            2082|  17|\n",
      "|               31217|            1997|  19|\n",
      "|               31217|            1959|  19|\n",
      "+--------------------+----------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RANK() is like ROW_NUMBER() but not exclusive.  There can be more than one of the same rank.\n",
    "# Below, there are duplicate start times (the ORDER BY) and so duplicate rank.\n",
    "# RANK() - 1, 2, 2, 4, 5, 5, 7....\n",
    "# DENSE_RANK() - 1, 2, 2, 3, 4, 4, 5.... (no skipping ranks)\n",
    "\n",
    "result = spark.sql('''\n",
    "   SELECT start_station_number,\n",
    "          duration_seconds,\n",
    "          RANK() OVER (PARTITION BY start_station_number ORDER BY start_time) AS rank\n",
    "   FROM bike_data\n",
    "   WHERE start_time < '2012-01-08'\n",
    "   ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTILE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
