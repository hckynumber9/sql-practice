{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mode.com/sql-tutorial/sql-window-functions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woot\n"
     ]
    }
   ],
   "source": [
    "# Establish Spark session\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[2]\") \\\n",
    "            .appName(\"df lecture\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext \n",
    "print(\"woot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SQL Window Functions** ##\n",
    "* Start spark session\n",
    "* Execute SQL queries\n",
    "* Next: Add notes and continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/jovyan/work/DS/repos_practice/sql-practice/misc/data/sales.csv;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bbda8d7e63eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m df_sales = spark.read.csv('data/sales.csv',\n\u001b[0m\u001b[1;32m      3\u001b[0m                          \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;31m# use headers or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0mquote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0;31m# char for quotes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0;31m# char for separation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/jovyan/work/DS/repos_practice/sql-practice/misc/data/sales.csv;"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_sales = spark.read.csv('data/sales.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "# Now create an SQL table and issue SQL queries against it without\n",
    "# using the sqlContext but through the SparkSession object.\n",
    "# Creates a temporary view of the DataFrame\n",
    "df_sales.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql('''\n",
    "    SELECT state, AVG(amount) as avg_amount\n",
    "    FROM sales\n",
    "    GROUP BY state\n",
    "    ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV in a Spark dataframe\n",
    "\n",
    "df_bike = spark.read.csv('data/2012Q1-capitalbikeshare-tripdata.csv', header=True, quote='\"', sep=',', inferSchema=False)\n",
    "df_bike.createOrReplaceTempView('bike_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+-----------+--------------------+--------------------+------------------+--------------------+-----------+-----------+\n",
      "|duration_seconds| start_time|   end_time|start_station_number|       start_station|end_station_number|         end_station|bike_number|member_type|\n",
      "+----------------+-----------+-----------+--------------------+--------------------+------------------+--------------------+-----------+-----------+\n",
      "|             475|1/1/12 0:04|1/1/12 0:11|               31245|7th & R St NW / S...|             31109|       7th & T St NW|     W01412|     Member|\n",
      "|            1162|1/1/12 0:10|1/1/12 0:29|               31400|Georgia & New Ham...|             31103|16th & Harvard St NW|     W00524|     Casual|\n",
      "|            1145|1/1/12 0:10|1/1/12 0:29|               31400|Georgia & New Ham...|             31103|16th & Harvard St NW|     W00235|     Member|\n",
      "|             485|1/1/12 0:15|1/1/12 0:23|               31101|      14th & V St NW|             31602|Park Rd & Holmead...|     W00864|     Member|\n",
      "|             471|1/1/12 0:15|1/1/12 0:23|               31102| 11th & Kenyon St NW|             31109|       7th & T St NW|     W00995|     Member|\n",
      "+----------------+-----------+-----------+--------------------+--------------------+------------------+--------------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check to make sure it worked\n",
    "\n",
    "result = spark.sql('''\n",
    "    SELECT * FROM bike_data LIMIT 5\n",
    "    ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+\n",
      "|duration_seconds|running_total|\n",
      "+----------------+-------------+\n",
      "|             475|        475.0|\n",
      "|            1162|       2782.0|\n",
      "|            1145|       2782.0|\n",
      "|             485|       3738.0|\n",
      "|             471|       3738.0|\n",
      "|             358|       4096.0|\n",
      "|            1754|       5850.0|\n",
      "|             259|       6109.0|\n",
      "|             516|       6625.0|\n",
      "|             913|       7538.0|\n",
      "|            1097|       8635.0|\n",
      "|             490|       9125.0|\n",
      "|            1045|      11205.0|\n",
      "|            1035|      11205.0|\n",
      "|            1060|      14063.0|\n",
      "|            1039|      14063.0|\n",
      "|             443|      14063.0|\n",
      "|             316|      14063.0|\n",
      "|             506|      14569.0|\n",
      "|             956|      15525.0|\n",
      "+----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate basic window function.  'OVER' designates the window function, ordered by start_time.\n",
    "\n",
    "result = spark.sql('''\n",
    "    SELECT duration_seconds,\n",
    "       SUM(duration_seconds) OVER (ORDER BY start_time) AS running_total\n",
    "    FROM bike_data\n",
    "    ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-------------+\n",
      "|start_station_number|duration_seconds|running_total|\n",
      "+--------------------+----------------+-------------+\n",
      "|               31217|             841|       1613.0|\n",
      "|               31217|             772|       1613.0|\n",
      "|               31217|            1623|       3236.0|\n",
      "|               31217|            1260|       5751.0|\n",
      "|               31217|            1255|       5751.0|\n",
      "|               31217|            5154|      12076.0|\n",
      "|               31217|            1171|      12076.0|\n",
      "|               31217|            4880|      16956.0|\n",
      "|               31217|             531|      17487.0|\n",
      "|               31217|            8831|      26318.0|\n",
      "|               31217|            8684|      35002.0|\n",
      "|               31217|            8681|      43683.0|\n",
      "|               31217|            8528|      52211.0|\n",
      "|               31217|             881|      53092.0|\n",
      "|               31217|             858|      53950.0|\n",
      "|               31217|            3029|      56979.0|\n",
      "|               31217|            2097|      61158.0|\n",
      "|               31217|            2082|      61158.0|\n",
      "|               31217|            1997|      65114.0|\n",
      "|               31217|            1959|      65114.0|\n",
      "+--------------------+----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PARTITION BY separates by starting station, then performs running_total ordered by start_time.  running_total starts over at each new station.\n",
    "\n",
    "result = spark.sql('''\n",
    "   SELECT start_station_number,\n",
    "       duration_seconds,\n",
    "       SUM(duration_seconds) OVER\n",
    "         (PARTITION BY start_station_number ORDER BY start_time)\n",
    "         AS running_total\n",
    "   FROM bike_data\n",
    "   WHERE start_time < '2012-01-08'\n",
    "   ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-------------+\n",
      "|start_station_number|duration_seconds|running_total|\n",
      "+--------------------+----------------+-------------+\n",
      "|               31217|             841|    2726242.0|\n",
      "|               31217|             772|    2726242.0|\n",
      "|               31217|            1623|    2726242.0|\n",
      "|               31217|            1260|    2726242.0|\n",
      "|               31217|            1255|    2726242.0|\n",
      "|               31217|            5154|    2726242.0|\n",
      "|               31217|            1171|    2726242.0|\n",
      "|               31217|            4880|    2726242.0|\n",
      "|               31217|             531|    2726242.0|\n",
      "|               31217|            8831|    2726242.0|\n",
      "|               31217|            8684|    2726242.0|\n",
      "|               31217|            8681|    2726242.0|\n",
      "|               31217|            8528|    2726242.0|\n",
      "|               31217|             881|    2726242.0|\n",
      "|               31217|             858|    2726242.0|\n",
      "|               31217|            3029|    2726242.0|\n",
      "|               31217|            2097|    2726242.0|\n",
      "|               31217|            2082|    2726242.0|\n",
      "|               31217|            1997|    2726242.0|\n",
      "|               31217|            1959|    2726242.0|\n",
      "+--------------------+----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Without ORDER BY, running_total is the total of all seconds (see below without ORDER BY).  ORDER BY moves row by row in order of the designated column.  \n",
    "# ORDER BY and PARTITION define the window.  Can't include window function in a GROUP BY clause.\n",
    "\n",
    "result = spark.sql('''\n",
    "    SELECT start_station_number,\n",
    "       duration_seconds,\n",
    "       SUM(duration_seconds) OVER\n",
    "         (PARTITION BY start_station_number)\n",
    "         AS running_total\n",
    "   FROM bike_data\n",
    "   WHERE start_time < '2012-01-08'\n",
    "    ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-------------+-------------+------------------+\n",
      "|start_station_number|duration_seconds|running_total|running_count|       running_avg|\n",
      "+--------------------+----------------+-------------+-------------+------------------+\n",
      "|               31217|             841|       1613.0|            2|             806.5|\n",
      "|               31217|             772|       1613.0|            2|             806.5|\n",
      "|               31217|            1623|       3236.0|            3|1078.6666666666667|\n",
      "|               31217|            1260|       5751.0|            5|            1150.2|\n",
      "|               31217|            1255|       5751.0|            5|            1150.2|\n",
      "|               31217|            5154|      12076.0|            7| 1725.142857142857|\n",
      "|               31217|            1171|      12076.0|            7| 1725.142857142857|\n",
      "|               31217|            4880|      16956.0|            8|            2119.5|\n",
      "|               31217|             531|      17487.0|            9|            1943.0|\n",
      "|               31217|            8831|      26318.0|           10|            2631.8|\n",
      "|               31217|            8684|      35002.0|           11|            3182.0|\n",
      "|               31217|            8681|      43683.0|           12|           3640.25|\n",
      "|               31217|            8528|      52211.0|           13| 4016.230769230769|\n",
      "|               31217|             881|      53092.0|           14| 3792.285714285714|\n",
      "|               31217|             858|      53950.0|           15|3596.6666666666665|\n",
      "|               31217|            3029|      56979.0|           16|         3561.1875|\n",
      "|               31217|            2097|      61158.0|           18|3397.6666666666665|\n",
      "|               31217|            2082|      61158.0|           18|3397.6666666666665|\n",
      "|               31217|            1997|      65114.0|           20|            3255.7|\n",
      "|               31217|            1959|      65114.0|           20|            3255.7|\n",
      "+--------------------+----------------+-------------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using aggregates with window functions.\n",
    "\n",
    "result = spark.sql('''\n",
    "    SELECT start_station_number,\n",
    "           duration_seconds,\n",
    "           SUM(duration_seconds) OVER\n",
    "               (PARTITION BY start_station_number ORDER BY start_time) AS running_total,\n",
    "           COUNT(duration_seconds) OVER\n",
    "               (PARTITION BY start_station_number ORDER BY start_time) AS running_count,\n",
    "           AVG(duration_seconds) OVER\n",
    "               (PARTITION BY start_station_number ORDER BY start_time) AS running_avg\n",
    "    FROM bike_data\n",
    "    WHERE start_time < '2012-01-08'\n",
    "     ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------------+----------+\n",
      "|start_station_number| start_time|duration_seconds|row_number|\n",
      "+--------------------+-----------+----------------+----------+\n",
      "|               31245|1/1/12 0:04|             475|         1|\n",
      "|               31400|1/1/12 0:10|            1162|         2|\n",
      "|               31400|1/1/12 0:10|            1145|         3|\n",
      "|               31101|1/1/12 0:15|             485|         4|\n",
      "|               31102|1/1/12 0:15|             471|         5|\n",
      "|               31017|1/1/12 0:17|             358|         6|\n",
      "|               31236|1/1/12 0:18|            1754|         7|\n",
      "|               31101|1/1/12 0:22|             259|         8|\n",
      "|               31014|1/1/12 0:24|             516|         9|\n",
      "|               31101|1/1/12 0:25|             913|        10|\n",
      "|               31303|1/1/12 0:29|            1097|        11|\n",
      "|               31222|1/1/12 0:30|             490|        12|\n",
      "|               31230|1/1/12 0:32|            1045|        13|\n",
      "|               31107|1/1/12 0:32|            1035|        14|\n",
      "|               31107|1/1/12 0:33|            1060|        15|\n",
      "|               31230|1/1/12 0:33|            1039|        16|\n",
      "|               31237|1/1/12 0:33|             443|        17|\n",
      "|               31109|1/1/12 0:33|             316|        18|\n",
      "|               31203|1/1/12 0:34|             506|        19|\n",
      "|               31214|1/1/12 0:36|             956|        20|\n",
      "+--------------------+-----------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ROW_NUMBER() displays row numbers according to ORDER BY.  PARTITION BY will cause row numbers to reset at the partition.\n",
    "\n",
    "result = spark.sql('''\n",
    "   SELECT start_station_number,\n",
    "          start_time,\n",
    "          duration_seconds,\n",
    "          ROW_NUMBER() OVER (ORDER BY start_time) AS row_number\n",
    "   FROM bike_data\n",
    "   WHERE start_time < '2012-01-08'\n",
    "   ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+----+\n",
      "|start_station_number|duration_seconds|rank|\n",
      "+--------------------+----------------+----+\n",
      "|               31217|             841|   1|\n",
      "|               31217|             772|   1|\n",
      "|               31217|            1623|   3|\n",
      "|               31217|            1260|   4|\n",
      "|               31217|            1255|   4|\n",
      "|               31217|            5154|   6|\n",
      "|               31217|            1171|   6|\n",
      "|               31217|            4880|   8|\n",
      "|               31217|             531|   9|\n",
      "|               31217|            8831|  10|\n",
      "|               31217|            8684|  11|\n",
      "|               31217|            8681|  12|\n",
      "|               31217|            8528|  13|\n",
      "|               31217|             881|  14|\n",
      "|               31217|             858|  15|\n",
      "|               31217|            3029|  16|\n",
      "|               31217|            2097|  17|\n",
      "|               31217|            2082|  17|\n",
      "|               31217|            1997|  19|\n",
      "|               31217|            1959|  19|\n",
      "+--------------------+----------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RANK() is like ROW_NUMBER() but not exclusive.  There can be more than one of the same rank.\n",
    "# Below, there are duplicate start times (the ORDER BY) and so duplicate rank.\n",
    "# RANK() - 1, 2, 2, 4, 5, 5, 7....\n",
    "# DENSE_RANK() - 1, 2, 2, 3, 4, 4, 5.... (no skipping ranks)\n",
    "\n",
    "result = spark.sql('''\n",
    "   SELECT start_station_number,\n",
    "          duration_seconds,\n",
    "          RANK() OVER (PARTITION BY start_station_number ORDER BY start_time) AS rank\n",
    "   FROM bike_data\n",
    "   WHERE start_time < '2012-01-08'\n",
    "   ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------+--------+----------+\n",
      "|start_station_number|duration_seconds|quartile|quintile|percentile|\n",
      "+--------------------+----------------+--------+--------+----------+\n",
      "|               31217|            1000|       1|       1|         1|\n",
      "|               31217|            1001|       1|       1|         1|\n",
      "|               31217|            1004|       1|       1|         1|\n",
      "|               31217|            1004|       1|       1|         1|\n",
      "|               31217|            1005|       1|       1|         1|\n",
      "|               31217|            1005|       1|       1|         1|\n",
      "|               31217|            1006|       1|       1|         1|\n",
      "|               31217|            1009|       1|       1|         1|\n",
      "|               31217|            1010|       1|       1|         1|\n",
      "|               31217|            1011|       1|       1|         1|\n",
      "|               31217|            1011|       1|       1|         1|\n",
      "|               31217|            1012|       1|       1|         1|\n",
      "|               31217|            1012|       1|       1|         1|\n",
      "|               31217|            1015|       1|       1|         1|\n",
      "|               31217|            1016|       1|       1|         1|\n",
      "|               31217|            1019|       1|       1|         1|\n",
      "|               31217|            1020|       1|       1|         1|\n",
      "|               31217|            1021|       1|       1|         1|\n",
      "|               31217|            1024|       1|       1|         1|\n",
      "|               31217|            1025|       1|       1|         1|\n",
      "+--------------------+----------------+--------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NTILE\n",
    "# The syntax is NTILE(*# of buckets*). In this case, ORDER BY determines which column to use to determine the quartiles (or whatever number of 'tiles you specify).\n",
    "\n",
    "result = spark.sql('''\n",
    "   SELECT start_station_number, duration_seconds,\n",
    "          NTILE(4) OVER\n",
    "             (PARTITION BY start_station_number ORDER BY duration_seconds) AS quartile,\n",
    "          NTILE(5) OVER\n",
    "             (PARTITION BY start_station_number ORDER BY duration_seconds) AS quintile,\n",
    "          NTILE(100) OVER\n",
    "             (PARTITION BY start_station_number ORDER BY duration_seconds) AS percentile\n",
    "  FROM bike_data\n",
    "  WHERE start_time < '2012-01-08'\n",
    "   ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+----+----+\n",
      "|start_station_number|duration_seconds| lag|lead|\n",
      "+--------------------+----------------+----+----+\n",
      "|               31000|             103|null| 127|\n",
      "|               31000|             127| 103| 128|\n",
      "|               31000|             128| 127|1327|\n",
      "|               31000|            1327| 128|1379|\n",
      "|               31000|            1379|1327|1422|\n",
      "|               31000|            1422|1379|1465|\n",
      "|               31000|            1465|1422|1474|\n",
      "|               31000|            1474|1465|1476|\n",
      "|               31000|            1476|1474|1491|\n",
      "|               31000|            1491|1476|1498|\n",
      "|               31000|            1498|1491|1505|\n",
      "|               31000|            1505|1498|1536|\n",
      "|               31000|            1536|1505|1555|\n",
      "|               31000|            1555|1536|1568|\n",
      "|               31000|            1568|1555|1586|\n",
      "|               31000|            1586|1568|1598|\n",
      "|               31000|            1598|1586|1602|\n",
      "|               31000|            1602|1598|1608|\n",
      "|               31000|            1608|1602|1614|\n",
      "|               31000|            1614|1608|1622|\n",
      "+--------------------+----------------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LAG and LEAD\n",
    "# LAG pulls from previous rows and LEAD pulls from following rows and creates new column.\n",
    "\n",
    "result = spark.sql('''\n",
    "   SELECT start_station_number, duration_seconds,\n",
    "          LAG(duration_seconds, 1) OVER\n",
    "            (PARTITION BY start_station_number ORDER BY duration_seconds) AS lag,\n",
    "          LEAD(duration_seconds, 1) OVER\n",
    "            (PARTITION BY start_station_number ORDER BY duration_seconds) AS lead\n",
    "  FROM bike_data\n",
    "  WHERE start_time < '2012-01-08'\n",
    "  ORDER BY 1, 2\n",
    "   ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+----------+\n",
      "|start_station_number|duration_seconds|difference|\n",
      "+--------------------+----------------+----------+\n",
      "|               31000|             103|      null|\n",
      "|               31000|             127|      24.0|\n",
      "|               31000|             128|       1.0|\n",
      "|               31000|            1327|    1199.0|\n",
      "|               31000|            1379|      52.0|\n",
      "|               31000|            1422|      43.0|\n",
      "|               31000|            1465|      43.0|\n",
      "|               31000|            1474|       9.0|\n",
      "|               31000|            1476|       2.0|\n",
      "|               31000|            1491|      15.0|\n",
      "|               31000|            1498|       7.0|\n",
      "|               31000|            1505|       7.0|\n",
      "|               31000|            1536|      31.0|\n",
      "|               31000|            1555|      19.0|\n",
      "|               31000|            1568|      13.0|\n",
      "|               31000|            1586|      18.0|\n",
      "|               31000|            1598|      12.0|\n",
      "|               31000|            1602|       4.0|\n",
      "|               31000|            1608|       6.0|\n",
      "|               31000|            1614|       6.0|\n",
      "+--------------------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LAG and LEAD\n",
    "# Useful when calculating differences between rows\n",
    "\n",
    "result = spark.sql('''\n",
    "   SELECT start_station_number, duration_seconds,\n",
    "          duration_seconds - LAG(duration_seconds, 1) OVER\n",
    "            (PARTITION BY start_station_number ORDER BY duration_seconds) AS difference\n",
    "   FROM bike_data\n",
    "   WHERE start_time < '2012-01-08'\n",
    "   ORDER BY 1, 2\n",
    "   ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+----------+\n",
      "|start_station_number|duration_seconds|difference|\n",
      "+--------------------+----------------+----------+\n",
      "|               31000|             127|      24.0|\n",
      "|               31000|             128|       1.0|\n",
      "|               31000|            1327|    1199.0|\n",
      "|               31000|            1379|      52.0|\n",
      "|               31000|            1422|      43.0|\n",
      "|               31000|            1465|      43.0|\n",
      "|               31000|            1474|       9.0|\n",
      "|               31000|            1476|       2.0|\n",
      "|               31000|            1491|      15.0|\n",
      "|               31000|            1498|       7.0|\n",
      "|               31000|            1505|       7.0|\n",
      "|               31000|            1536|      31.0|\n",
      "|               31000|            1555|      19.0|\n",
      "|               31000|            1568|      13.0|\n",
      "|               31000|            1586|      18.0|\n",
      "|               31000|            1598|      12.0|\n",
      "|               31000|            1602|       4.0|\n",
      "|               31000|            1608|       6.0|\n",
      "|               31000|            1614|       6.0|\n",
      "|               31000|            1622|       8.0|\n",
      "+--------------------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LAG and LEAD\n",
    "# Wrap in outer query to remove nulls, like above.\n",
    "\n",
    "result = spark.sql('''\n",
    "   SELECT *\n",
    "   FROM (\n",
    "     SELECT start_station_number, duration_seconds,\n",
    "          duration_seconds - LAG(duration_seconds, 1) OVER\n",
    "            (PARTITION BY start_station_number ORDER BY duration_seconds) AS difference\n",
    "     FROM bike_data\n",
    "     WHERE start_time < '2012-01-08'\n",
    "     ORDER BY 1, 2\n",
    "        ) sub\n",
    "   WHERE sub.difference IS NOT NULL\n",
    "   ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------+--------+----------+\n",
      "|start_station_number|duration_seconds|quartile|quintile|percentile|\n",
      "+--------------------+----------------+--------+--------+----------+\n",
      "|               31000|             103|       1|       1|         1|\n",
      "|               31000|             127|       1|       1|         1|\n",
      "|               31000|             128|       1|       1|         2|\n",
      "|               31000|            1327|       1|       1|         2|\n",
      "|               31000|            1379|       1|       1|         3|\n",
      "|               31000|            1422|       1|       1|         3|\n",
      "|               31000|            1465|       1|       1|         4|\n",
      "|               31000|            1474|       1|       1|         4|\n",
      "|               31000|            1476|       1|       1|         5|\n",
      "|               31000|            1491|       1|       1|         5|\n",
      "|               31000|            1498|       1|       1|         6|\n",
      "|               31000|            1505|       1|       1|         6|\n",
      "|               31000|            1536|       1|       1|         7|\n",
      "|               31000|            1555|       1|       1|         7|\n",
      "|               31000|            1568|       1|       1|         8|\n",
      "|               31000|            1586|       1|       1|         8|\n",
      "|               31000|            1598|       1|       1|         9|\n",
      "|               31000|            1602|       1|       1|         9|\n",
      "|               31000|            1608|       1|       1|        10|\n",
      "|               31000|            1614|       1|       1|        10|\n",
      "+--------------------+----------------+--------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Window alias\n",
    "# NTILE example above can be rewritten as:\n",
    "\n",
    "result = spark.sql('''\n",
    "   SELECT start_station_number, duration_seconds,\n",
    "          NTILE(4) OVER ntile_window AS quartile,\n",
    "          NTILE(5) OVER ntile_window AS quintile,\n",
    "          NTILE(100) OVER ntile_window AS percentile\n",
    "  FROM bike_data\n",
    "  WHERE start_time < '2012-01-08'\n",
    "  WINDOW ntile_window AS (PARTITION BY start_station_number ORDER BY duration_seconds)\n",
    "  ORDER BY 1, 2\n",
    "   ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
