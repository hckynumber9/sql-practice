{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woot\n"
     ]
    }
   ],
   "source": [
    "# Establish Spark session\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[2]\") \\\n",
    "            .appName(\"df lecture\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext \n",
    "print(\"woot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SQL Window Functions** ##\n",
    "* Start spark session\n",
    "* Execute SQL queries\n",
    "* Next: Add notes and continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV\n",
    "df_sales = spark.read.csv('data/sales.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "# Now create an SQL table and issue SQL queries against it without\n",
    "# using the sqlContext but through the SparkSession object.\n",
    "# Creates a temporary view of the DataFrame\n",
    "df_sales.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql('''\n",
    "    SELECT state, AVG(amount) as avg_amount\n",
    "    FROM sales\n",
    "    GROUP BY state\n",
    "    ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bike = spark.read.csv('data/2012Q1-capitalbikeshare-tripdata.csv', header=True, quote='\"', sep=',', inferSchema=False)\n",
    "df_bike.createOrReplaceTempView('bike_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+-----------+--------------------+--------------------+------------------+--------------------+-----------+-----------+\n",
      "|duration_seconds| start_time|   end_time|start_station_number|       start_station|end_station_number|         end_station|bike_number|member_type|\n",
      "+----------------+-----------+-----------+--------------------+--------------------+------------------+--------------------+-----------+-----------+\n",
      "|             475|1/1/12 0:04|1/1/12 0:11|               31245|7th & R St NW / S...|             31109|       7th & T St NW|     W01412|     Member|\n",
      "|            1162|1/1/12 0:10|1/1/12 0:29|               31400|Georgia & New Ham...|             31103|16th & Harvard St NW|     W00524|     Casual|\n",
      "|            1145|1/1/12 0:10|1/1/12 0:29|               31400|Georgia & New Ham...|             31103|16th & Harvard St NW|     W00235|     Member|\n",
      "|             485|1/1/12 0:15|1/1/12 0:23|               31101|      14th & V St NW|             31602|Park Rd & Holmead...|     W00864|     Member|\n",
      "|             471|1/1/12 0:15|1/1/12 0:23|               31102| 11th & Kenyon St NW|             31109|       7th & T St NW|     W00995|     Member|\n",
      "+----------------+-----------+-----------+--------------------+--------------------+------------------+--------------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "    SELECT * FROM bike_data LIMIT 5\n",
    "    ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+\n",
      "|duration_seconds|running_total|\n",
      "+----------------+-------------+\n",
      "|             475|        475.0|\n",
      "|            1162|       2782.0|\n",
      "|            1145|       2782.0|\n",
      "|             485|       3738.0|\n",
      "|             471|       3738.0|\n",
      "|             358|       4096.0|\n",
      "|            1754|       5850.0|\n",
      "|             259|       6109.0|\n",
      "|             516|       6625.0|\n",
      "|             913|       7538.0|\n",
      "|            1097|       8635.0|\n",
      "|             490|       9125.0|\n",
      "|            1045|      11205.0|\n",
      "|            1035|      11205.0|\n",
      "|            1060|      14063.0|\n",
      "|            1039|      14063.0|\n",
      "|             443|      14063.0|\n",
      "|             316|      14063.0|\n",
      "|             506|      14569.0|\n",
      "|             956|      15525.0|\n",
      "+----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "    SELECT duration_seconds,\n",
    "       SUM(duration_seconds) OVER (ORDER BY start_time) AS running_total\n",
    "    FROM bike_data\n",
    "    ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-------------+\n",
      "|start_station_number|duration_seconds|running_total|\n",
      "+--------------------+----------------+-------------+\n",
      "|               31217|             841|       1613.0|\n",
      "|               31217|             772|       1613.0|\n",
      "|               31217|            1623|       3236.0|\n",
      "|               31217|            1260|       5751.0|\n",
      "|               31217|            1255|       5751.0|\n",
      "|               31217|            5154|      12076.0|\n",
      "|               31217|            1171|      12076.0|\n",
      "|               31217|            4880|      16956.0|\n",
      "|               31217|             531|      17487.0|\n",
      "|               31217|            8831|      26318.0|\n",
      "|               31217|            8684|      35002.0|\n",
      "|               31217|            8681|      43683.0|\n",
      "|               31217|            8528|      52211.0|\n",
      "|               31217|             881|      53092.0|\n",
      "|               31217|             858|      53950.0|\n",
      "|               31217|            3029|      56979.0|\n",
      "|               31217|            2097|      61158.0|\n",
      "|               31217|            2082|      61158.0|\n",
      "|               31217|            1997|      65114.0|\n",
      "|               31217|            1959|      65114.0|\n",
      "+--------------------+----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "   SELECT start_station_number,\n",
    "       duration_seconds,\n",
    "       SUM(duration_seconds) OVER\n",
    "         (PARTITION BY start_station_number ORDER BY start_time)\n",
    "         AS running_total\n",
    "   FROM bike_data\n",
    "   WHERE start_time < '2012-01-08'\n",
    "   ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
